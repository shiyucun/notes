
# 回归模型

- [回归模型](#回归模型)
  - [误差分析](#误差分析)
  - [线性回归](#线性回归)
  - [岭回归](#岭回归)
  - [多项式回归](#多项式回归)
  - [决策树回归和自适应增强决策树回归](#决策树回归和自适应增强决策树回归)

## 误差分析

一种模型一套评估指标，回归模型通常参照以下指标(其中得分越高越好，误差越小越好)：

- 平均绝对值误差(Mean Absolute Error)：
  - 所有数据点的误差绝对值的平均数
- 均方误差(Mean Squared Error)：
  - 所有数据点的误差平方值的平均数，不破坏误差数据本身的连续性规则
- 中位数绝对值误差(Median Absolute Error)：
  - 所有数据点的误差绝对值的中位数，有利于排除某些奇异值(Outlier)的干扰
- 解释方差得分(Explained Variance Score)：
  - 反映模型对数据集波动性的解释能力。[0,1]，如果得分为1，则模型十分完美
- R方得分(R2 Score)：
  - 反映模型对未知数据的预测能力。[-1,1]，如果得分为1，则模型十分完美，可能为负

## 线性回归

回归是估计输入数据与输出数据之间的关系。

线性回归即假定输入数据和输出数据之间符合线性关系：`y=kx+b`。

训练模型的意义就在于根据大量的已知样本`[x,y]`，按照最小二乘法找到适当的线性参数`[k,b]`。

```python
import os
import sys
import platform
import pickle
import numpy as np
import sklearn.linear_model as sl
import sklearn.metrics as sm
import matplotlib.pyplot as mp
import matplotlib.patches as mc


def read_data(filename):
    x, y = [], []
    with open(filename, 'r') as f:
        for line in f.readlines():
            data = [float(substr) for substr in line.split(',')]
            x.append(data[:-1])
            y.append(data[-1])
    return np.array(x), np.array(y)


def train_model(x, y):
    model = sl.LinearRegression()  # 使用线性回归模型
    model.fit(x, y)  # 训练模型
    return model


def pred_model(model, x):
    y = model.predict(x)
    return y


def eval_model(y, pred_y):  # 评价模型
    mae = sm.mean_absolute_error(y, pred_y)
    mse = sm.mean_squared_error(y, pred_y)
    mda = sm.median_absolute_error(y, pred_y)
    evs = sm.explained_variance_score(y, pred_y)
    r2s = sm.r2_score(y, pred_y)
    print(round(mae, 2), round(mse, 2), round(mda, 2), round(evs, 2), round(r2s, 2))


def save_model(model, filename):
    with open(filename, 'wb') as f:
        pickle.dump(model, f)


def init_chart():
    mp.gcf().set_facecolor(np.ones(3) * 240 / 255)
    mp.title('Linear Regression', fontsize=20)
    mp.xlabel('x', fontsize=14)
    mp.ylabel('y', fontsize=14)
    mp.tick_params(which='both', top=True, right=True, labelright=True, labelsize=10)
    mp.grid(linestyle=':')


def draw_train(train_x, train_y, pred_train_y):
    mp.plot(train_x, train_y, 's', c='limegreen', label='Training')
    sorted_indices = train_x.T[0].argsort()
    mp.plot(train_x.T[0][sorted_indices],
            pred_train_y[sorted_indices], '--', c='dodgerblue',
            label='Predicted Training')
    mp.legend()


def draw_test(test_x, test_y, pred_test_y):
    mp.plot(test_x, test_y, 's', c='orangered', label='Testing')
    mp.plot(test_x, pred_test_y, 'o', c='orangered', label='Predicted Testing')
    for x, pred_y, y in zip(test_x, pred_test_y, test_y):
        mp.gca().add_patch(mc.Arrow(x, pred_y, 0, y - pred_y, width=0.8, ec='none', fc='pink'))
    mp.legend()


def show_chart():
    mng = mp.get_current_fig_manager()
    # if 'Windows' in platform.system():
    #     mng.window.state('zoomed')
    # else:
    #     mng.resize(*mng.window.maxsize())
    mp.show()


def main(argc, argv, envp):
    # 读取single.txt，获取训练集和测试集数据
    # 文件single.txt中以"x,y"的格式存储了共50条数据，其中每条数据占一行
    x, y = read_data('single.txt')
    print(x.shape, y.shape)

    # 读取的数据分为训练集和测试集，一般按照4:1或7:3的比例划分，以防止过拟合(训练集比例高)、欠拟合(测试集比例高)
    # 训练集数据
    train_size = int(x.size * 0.8)
    train_x = x[:train_size]
    train_y = y[:train_size]
    # 训练
    model = train_model(train_x, train_y)
    # 训练集数据输入模型得到结果用于后续画图
    pred_train_y = pred_model(model, train_x)

    # 测试集数据
    test_x = x[train_size:]
    test_y = y[train_size:]
    # 测试集数据输入模型得到预测的结果
    pred_test_y = pred_model(model, test_x)

    # 输入测试集数据和预测结果评价模型
    eval_model(test_y, pred_test_y)

    # 保存model，共后续使用
    save_model(model, 'linear.mod')

    # 画图
    init_chart()
    draw_train(train_x, train_y, pred_train_y)
    draw_test(test_x, test_y, pred_test_y)
    show_chart()

    return 0

if __name__ == '__main__':
    sys.exit(main(len(sys.argv), sys.argv, os.environ))
```

## 岭回归

线性回归的主要问题是对异常值过于敏感。

在真实世界的数据收集过程中，经常会遇到错误的度量结果，而线性回归使用普通的最小二乘法，其目标是使每个样本的误差平方最小。

这时，由于异常值误差的绝对值通常较大，会引起回归模型质量的下降。

为了避免以上问题，可以引入包含正则化系数(α)的岭回归模型，通过阈值和权重来有选择地削弱异常样本对回归效果的影响。

```python
import os
import sys
import platform
import numpy as np
import sklearn.linear_model as sl
import sklearn.metrics as sm
import matplotlib.pyplot as mp
import matplotlib.patches as mc


def read_data(filename):
    x, y = [], []
    with open(filename, 'r') as f:
        for line in f.readlines():
            data = [float(substr) for substr in line.split(',')]
            x.append(data[:-1])
            y.append(data[-1])
    return np.array(x), np.array(y)


def train_model(alpha, x, y):
    model = sl.Ridge(alpha, fit_intercept=True, max_iter=10000)  # 岭回归模型
    model.fit(x, y)  # 训练
    return model


def pred_model(model, x):
    y = model.predict(x)
    return y


def eval_model(y, pred_y):
    mae = sm.mean_absolute_error(y, pred_y)
    mse = sm.mean_squared_error(y, pred_y)
    mde = sm.median_absolute_error(y, pred_y)
    evs = sm.explained_variance_score(y, pred_y)
    r2s = sm.r2_score(y, pred_y)
    print(mae, mse, mde, evs, r2s)


def init_chart():
    mp.gcf().set_facecolor(np.ones(3) * 240 / 255)
    mp.title('Ridge Regression', fontsize=20)
    mp.xlabel('x', fontsize=14)
    mp.ylabel('y', fontsize=14)
    mp.tick_params(which='both', top=True, right=True, labelright=True, labelsize=10)
    mp.grid(linestyle=':')


def draw_train(train_x, train_y, pred_train_y1, pred_train_y2):
    mp.plot(train_x, train_y, 's', c='limegreen', label='Training')
    sorted_indices = train_x.T[0].argsort()
    mp.plot(train_x.T[0][sorted_indices],
            pred_train_y1[sorted_indices], '--', c='lightskyblue',
            label='Predicted Training (α=0)')
    mp.plot(train_x.T[0][sorted_indices],
            pred_train_y2[sorted_indices], '--', c='dodgerblue',
            label='Predicted Training (α=150)')
    mp.legend()


def draw_test(test_x, test_y, pred_test_y1, pred_test_y2):
    mp.plot(test_x, test_y, 's', c='orangered', label='Testing')
    mp.plot(test_x, pred_test_y1, 'o', c='lightskyblue', label='Predicted Testing (α=0)')
    for x, pred_y, y in zip(test_x, pred_test_y1, test_y):
        mp.gca().add_patch(mc.Arrow(x, pred_y, 0, y - pred_y, width=0.8, ec='none', fc='pink'))
    mp.plot(test_x, pred_test_y2, 'o', c='dodgerblue', label='Predicted Testing (α=150)')
    for x, pred_y, y in zip(test_x, pred_test_y2, test_y):
        mp.gca().add_patch(mc.Arrow(x, pred_y, 0, y - pred_y, width=0.8, ec='none', fc='pink'))
    mp.legend()


def show_chart():
    mng = mp.get_current_fig_manager()
    # if 'Windows' in platform.system():
    #     mng.window.state('zoomed')
    # else:
    #     mng.resize(*mng.window.maxsize())
    mp.show()


def main(argc, argv, envp):
    x, y = read_data('abnormal.txt')

    train_size = int(x.size * 0.8)
    train_x = x[:train_size]
    train_y = y[:train_size]

    model1 = train_model(0, train_x, train_y)
    model2 = train_model(150, train_x, train_y)

    pred_train_y1 = pred_model(model1, train_x)
    pred_train_y2 = pred_model(model2, train_x)

    test_x = x[train_size:]
    test_y = y[train_size:]
    pred_test_y1 = pred_model(model1, test_x)
    pred_test_y2 = pred_model(model2, test_x)

    print('α=0')
    eval_model(test_y, pred_test_y1)

    print('α=150')
    eval_model(test_y, pred_test_y2)

    init_chart()
    draw_train(train_x, train_y, pred_train_y1, pred_train_y2)
    draw_test(test_x, test_y, pred_test_y1, pred_test_y2)
    show_chart()

    return 0


if __name__ == '__main__':
    sys.exit(main(len(sys.argv), sys.argv, os.environ))
```

## 多项式回归

在单因子（连续变量）试验中，当回归函数不能用直线来描述时，要考虑用非线性回归函数。

多项式回归属于非线性回归的一种，这里指单因子多项式回归，即一元多项式回归。

一般非线性回归函数是未知的，或即使已知也未必可以用一个简单的函数变换转化为线性模型。

这时，常用的做法是用因子的多项式：

- 如果从散点图观察到回归函数有一个弯，则可考虑用二次多项式
- 有两个弯则考虑用三次多项式
- 有三个弯则考虑用四次多项式
- ...

真实的回归函数未必就是某个次数的多项式，但只要拟合得好，用适当的多项式来近似真实的回归函数是可行的。

```python
import os
import sys
import platform
import numpy as np
import sklearn.pipeline as si
import sklearn.preprocessing as sp
import sklearn.linear_model as sl
import sklearn.metrics as sm
import matplotlib.pyplot as mp
import matplotlib.patches as mc


def read_data(filename):
    x, y = [], []
    with open(filename, 'r') as f:
        for line in f.readlines():
            data = [float(substr) for substr in line.split(',')]
            x.append(data[:-1])
            y.append(data[-1])
    return np.array(x), np.array(y)


def train_model(degree, x, y):
    model = si.make_pipeline(sp.PolynomialFeatures(degree), sl.LinearRegression())
    model.fit(x, y)
    return model


def pred_model(model, x):
    y = model.predict(x)
    return y


def eval_model(y, pred_y):
    mae = sm.mean_absolute_error(y, pred_y)
    mse = sm.mean_squared_error(y, pred_y)
    mde = sm.median_absolute_error(y, pred_y)
    evs = sm.explained_variance_score(y, pred_y)
    r2s = sm.r2_score(y, pred_y)
    print(mae, mse, mde, evs, r2s)


def init_chart():
    mp.gcf().set_facecolor(np.ones(3) * 240 / 255)
    mp.title('Polynomial Regression', fontsize=20)
    mp.xlabel('x', fontsize=14)
    mp.ylabel('y', fontsize=14)
    mp.tick_params(which='both', top=True, right=True, labelright=True, labelsize=10)
    mp.grid(linestyle=':')


def draw_train(train_x, train_y, pred_train_y):
    mp.plot(train_x, train_y, 's', c='limegreen', label='Training')
    sorted_indices = train_x.T[0].argsort()
    mp.plot(train_x.T[0][sorted_indices],
            pred_train_y[sorted_indices], '--', c='dodgerblue',
            label='Predicted Training')
    mp.legend()


def draw_test(test_x, test_y, pred_test_y):
    mp.plot(test_x, test_y, 's', c='orangered', label='Testing')
    mp.plot(test_x, pred_test_y, 'o', c='dodgerblue', label='Predicted Testing')
    for x, pred_y, y in zip(test_x, pred_test_y, test_y):
        mp.gca().add_patch(mc.Arrow(x, pred_y, 0, y - pred_y, width=0.8, ec='none', fc='pink'))
    mp.legend()


def show_chart():
    mng = mp.get_current_fig_manager()
    # if 'Windows' in platform.system():
    #     mng.window.state('zoomed')
    # else:
    #     mng.resize(*mng.window.maxsize())
    mp.show()


def main(argc, argv, envp):
    x, y = read_data('single.txt')

    train_size = int(x.size * 0.8)
    train_x = x[:train_size]
    train_y = y[:train_size]

    model = train_model(3, train_x, train_y)
    pred_train_y = pred_model(model, train_x)

    test_x = x[train_size:]
    test_y = y[train_size:]
    pred_test_y = pred_model(model, test_x)

    eval_model(test_y, pred_test_y)

    init_chart()
    draw_train(train_x, train_y, pred_train_y)
    draw_test(test_x, test_y, pred_test_y)
    show_chart()
    return 0


if __name__ == '__main__':
    sys.exit(main(len(sys.argv), sys.argv, os.environ))
```

## 决策树回归和自适应增强决策树回归

在一个特定的回归模型中，影响最终输出的诸多特性所做出的贡献并不相同。

评估其贡献大小有助于排除影响较小的因素，简化后续数据的处理过程。

```python
import os
import sys
import platform
import numpy as np
import sklearn.datasets as sd
import sklearn.utils as su
import sklearn.tree as st
import sklearn.ensemble as se
import sklearn.metrics as sm
import matplotlib.pyplot as mp


def read_data():
    housing = sd.load_boston()  # 在线加载波士顿房价数据
    fn = housing.feature_names
    print(fn)
    x, y = su.shuffle(housing.data, housing.target, random_state=7)  # 打乱数据
    return fn, x, y


def train_model_dt(x, y):
    model_dt = st.DecisionTreeRegressor(max_depth=4)  # 决策树模型
    model_dt.fit(x, y)  # 训练
    return model_dt


def train_model_ab(x, y):
    model_ab = se.AdaBoostRegressor(
        st.DecisionTreeRegressor(max_depth=4),
        n_estimators=400, random_state=7)  # 自适应增强决策树模型
    model_ab.fit(x, y)  # 训练
    return model_ab


def pred_model(model, x):
    y = model.predict(x)
    return y


def eval_model(y, pred_y):
    mae = sm.mean_absolute_error(y, pred_y)
    mse = sm.mean_squared_error(y, pred_y)
    mde = sm.median_absolute_error(y, pred_y)
    evs = sm.explained_variance_score(y, pred_y)
    r2s = sm.r2_score(y, pred_y)
    print(mae, mse, mde, evs, r2s)


def init_model_dt():
    mp.gcf().set_facecolor(np.ones(3) * 240 / 255)
    mp.subplot(211)
    mp.title('Decision Tree Regression', fontsize=16)
    mp.xlabel('Feature', fontsize=12)
    mp.ylabel('Importance', fontsize=12)
    mp.tick_params(which='both', top=True, right=True, labelright=True, labelsize=10)
    mp.grid(axis='y', linestyle=':')


def draw_model_dt(fn, fi_dt):
    fi_dt = (fi_dt * 100) / fi_dt.max()
    sorted_indices = np.flipud(fi_dt.argsort())
    pos = np.arange(sorted_indices.size)
    mp.bar(pos, fi_dt[sorted_indices], align='center',
           facecolor='deepskyblue', edgecolor='steelblue',
           label='Decision Tree')
    mp.xticks(pos, fn[sorted_indices])
    mp.legend()


def init_model_ab():
    mp.gcf().set_facecolor(np.ones(3) * 240 / 255)
    mp.subplot(212)
    mp.title('Ada Boost Decision Tree Regression', fontsize=16)
    mp.xlabel('Feature', fontsize=12)
    mp.ylabel('Importance', fontsize=12)
    mp.tick_params(which='both', top=True, right=True, labelright=True, labelsize=10)
    mp.grid(axis='y', linestyle=':')


def draw_model_ab(fn, fi_ab):
    fi_ab = (fi_ab * 100) / fi_ab.max()
    sorted_indices = np.flipud(fi_ab.argsort())
    pos = np.arange(sorted_indices.size)
    mp.bar(pos, fi_ab[sorted_indices], align='center',
           facecolor='lightcoral', edgecolor='indianred',
           label='Ada Boost Decision Tree')
    mp.xticks(pos, fn[sorted_indices])
    mp.legend()


def show_chart():
    mng = mp.get_current_fig_manager()
    # if 'Windows' in platform.system():
    #     mng.window.state('zoomed')
    # else:
    #     mng.resize(*mng.window.maxsize())
    mp.tight_layout()
    mp.show()


def main(argc, argv, envp):
    fn, x, y = read_data()

    train_size = int(len(x) * 0.8)
    train_x = x[:train_size]
    train_y = y[:train_size]

    model_dt = train_model_dt(train_x, train_y)
    model_ab = train_model_ab(train_x, train_y)

    test_x = x[train_size:]
    test_y = y[train_size:]
    pred_test_y_dt = pred_model(model_dt, test_x)
    pred_test_y_ab = pred_model(model_ab, test_x)

    eval_model(test_y, pred_test_y_dt)
    eval_model(test_y, pred_test_y_ab)

    fi_dt = model_dt.feature_importances_
    fi_ab = model_ab.feature_importances_

    init_model_dt()
    draw_model_dt(fn, fi_dt)
    init_model_ab()
    draw_model_ab(fn, fi_ab)
    show_chart()

    return 0


if __name__ == '__main__':
    sys.exit(main(len(sys.argv), sys.argv, os.environ))

# ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO' 'B' 'LSTAT']
# 2.76709759408 14.7900483922 2.36282051282 0.820600172129 0.820256088941
# 2.18758125088 7.64448730085 1.76050847458 0.907295798764 0.907096311719
# 由上面的结果可以看出自适应增强决策树模型比决策树模型的误差更小、评分更高
# 在柱状图中可以看到对于决策树模型'RM'的影响最大，对于自适应增强决策树模型'LSTAT'的影响最大
```
