# 自然语言处理

- [自然语言处理](#自然语言处理)
  - [数据预处理](#数据预处理)
    - [文本分割](#文本分割)
    - [词干提取](#词干提取)
    - [词形还原](#词形还原)
  - [词袋模型](#词袋模型)
  - [词频逆文档频率](#词频逆文档频率)

## 数据预处理

### 文本分割

数据预处理将文本分割为更小的易于被编码和数字化的形式。

可以按句子、词或词块分割。

```python
import nltk.tokenize as tk

doc = "Are you curious about tokenization? " \
    "Let's see how it works! " \
    "We need to analyze a couple of sentences " \
    "with punctuations to see it in action."
print(doc)
print('-' * 40)

tokens = tk.sent_tokenize(doc)
for token in tokens:
    print(token)
print('-' * 40)

tokens = tk.word_tokenize(doc)
for token in tokens:
    print(token)
print('-' * 40)

tokenizer = tk.WordPunctTokenizer()
tokens = tokenizer.tokenize(doc)
for token in tokens:
    print(token)
```

### 词干提取

在信息检索和文本挖掘中，需要对一个词的不同形态进行归并，即词形规范化，从而提高文本处理的效率。

例如：词根run有不同的形式running、ran另外runner也和run有关。这里涉及到两个概念：

- 词形变化：把一个任何形式的语言词汇还原为一般形式。(如：cats--->cat，did--->do)
- 词干提取：去除词缀得到词根的过程。(如：fisher--->fish，effective--->effect)

一般而言：

- Porter提干提取器最为宽松，复杂度降低能力有限
- Lancaster提取器又过于严苛，信息损失过多，导致模型的精度下降
- Snowball提取器的性能介于以上二者之间，最为常用

```python
import nltk.stem.porter as pt
import nltk.stem.lancaster as lc
import nltk.stem.snowball as sb

words = ['table', 'probably', 'wolves', 'playing', 'is',
         'dog', 'the', 'beaches', 'grounded', 'dreamt',
         'envision']

stemmer = pt.PorterStemmer()
for word in words:
    stem = stemmer.stem(word)
    print(word, '->', stem)
print('-' * 40)

stemmer = lc.LancasterStemmer()
for word in words:
    stem = stemmer.stem(word)
    print(word, '->', stem)
print('-' * 40)

stemmer = sb.SnowballStemmer('english')
for word in words:
    stem = stemmer.stem(word)
    print(word, '->', stem)
```

### 词形还原

分别针对名词和动词，利用语法规则上的差别，更有针对性地还原单词的原型。

```python
import nltk.stem as ns

words = ['table', 'probably', 'wolves', 'playing', 'is',
         'dog', 'the', 'beaches', 'grounded', 'dreamt',
         'envision']

lemmatizer = ns.WordNetLemmatizer()

for word in words:
    lemma = lemmatizer.lemmatize(word, pos='n')
    print(word, '->', lemma)
print('-' * 40)

lemmatizer = ns.WordNetLemmatizer()
for word in words:
    lemma = lemmatizer.lemmatize(word, pos='v')
    print(word, '->', lemma)
```

## 词袋模型

词袋就是一段文本中每句话中单词出现的频率矩阵。

借助词袋可以将文本信息有效地数字化，便于通过数学方法进行回归/分类/聚类等学习应用。

```python
import nltk.tokenize as tk
import sklearn.feature_extraction.text as ft

doc = 'The brown dog is running. ' \
    'The black dog is in the black room. ' \
    'Running in the room is forbidden.'
print(doc)
print('-' * 40)

sentences = tk.sent_tokenize(doc)
print(sentences)

cv = ft.CountVectorizer()
tfmat = cv.fit_transform(sentences).toarray()
words = cv.get_feature_names()
print(tfmat)
print(words)
```

## 词频逆文档频率

TF-IDF（term frequency–inverse document frequency）是一种用于资讯检索与资讯探勘的常用加权术。

TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。

字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。

TF-IDF加权的各种形式常被搜寻引擎应用，作为文件与用户查询之间相关程度的度量或评级。

```python
import sklearn.datasets as sd
import sklearn.feature_extraction.text as ft
import sklearn.naive_bayes as nb

cld = {'misc.forsale': 'SALES',
       'rec.motorcycles': 'MOTORCYCLES',
       'rec.sport.baseball': 'BASEBALL',
       'sci.crypt': 'CRYPTOGRAPHY',
       'sci.space': 'SPACE'}
train = sd.fetch_20newsgroups(
    subset='train', categories=cld.keys(), shuffle=True,
    random_state=7)

train_data = train.data
train_y = train.target
categories = train.target_names
print(len(train_data))
print(len(train_y))
print(len(categories))

cv = ft.CountVectorizer()
train_tfmat = cv.fit_transform(train_data)
print(train_tfmat.shape)

tf = ft.TfidfTransformer()
train_x = tf.fit_transform(train_tfmat)

model = nb.MultinomialNB()
model.fit(train_x, train_y)

test_data = [
    'The curveballs of right handed pitchers tend to curve to the left',
    'Caesar cipher is an ancient form of encryption',
    'This two-wheeler is really good on slippery roads']
test_tfmat = cv.transform(test_data)
test_x = tf.transform(test_tfmat)
pred_test_y = model.predict(test_x)

for sentence, index in zip(test_data, pred_test_y):
    print(sentence, '->', cld[categories[index]])
```
