
# 简单爬虫

- [简单爬虫](#简单爬虫)
  - [python2简单爬虫](#python2简单爬虫)
  - [python3简单爬虫](#python3简单爬虫)
  - [加入异常处理和重试处理](#加入异常处理和重试处理)

## python2简单爬虫

```python
# -*- coding: utf-8 -*-
import urllib2
import urlparse

def download(url):
    # 设置 user-agent
    user_agent = "Mozilla/5.0 (Windows NT 10.0; WOW64; rv:58.0) Gecko/20100101 Firefox/58.0"
    headers = {'User-agent': user_agent}
    # 使用 user-agent 发送 request 请求
    request = urllib2.Request(url, headers=headers)
    # 读取页面信息
    html = urllib2.urlopen(request).read()
    return html

print(download('https://www.baidu.com/'))
```

## python3简单爬虫

```python
# python 默认的 user-agent: python-requests/2.18.4

from urllib import request


# 设置 user-agent 和 url
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64; rv:45.0) Gecko/20100101 Firefox/45.0",}
url = 'https://www.baidu.com/'
# 发送 request 请求
req = request.Request(url, headers=headers)
response = request.urlopen(req)
print(type(response))  # <class 'http.client.HTTPResponse'>
# 打印响应信息
print(response.read().decode())
```

## 加入异常处理和重试处理

```python
# -*- coding: utf-8 -*-

import urllib2
import urlparse

def download(url, user_agent='wswp', proxy=None, num_retries=2):
    print 'Downloading:', url
    headers = {'User-agent': user_agent}
    request = urllib2.Request(url, headers=headers)
    opener = urllib2.build_opener()
    if proxy:
        proxy_params = {urlparse.urlparse(url).scheme: proxy}
        opener.add_handler(urllib2.ProxyHandler(proxy_params))
    try:
        html = opener.open(request).read()
    except urllib2.URLError as e:
        print 'Download error:', e.reason
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:  # 5xx为服务器错误
                # retry 5XX HTTP errors
                html = download(url, user_agent, proxy, num_retries-1)
    print(html)
    return html


if __name__ == '__main__':
    print download('https://www.baidu.com/')
```
